{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "repbrep = '../../'\n",
    "outdir = '../../figures/'\n",
    "results_dir = '../../results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import plot_classification_accuracies, plot_segmentation_accuracies\n",
    "import pandas as pd\n",
    "import altair_saver\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Segmentation Plots\n",
    "f360_segmentation = pd.read_parquet(os.path.join(repbrep, 'results/f360_segmentation.parquet'))\n",
    "mfcad_segmentation = pd.read_parquet(os.path.join(repbrep, 'results/mfcad_segmentation.parquet'))\n",
    "f360_accuracy_plot = plot_segmentation_accuracies(f360_segmentation,  'Fusion360Seg', 'macro', title='Fusion 360 Segmentation')\n",
    "mfcad_accuracy_plot = plot_segmentation_accuracies(mfcad_segmentation,  'MFCAD', 'macro', title='MFCAD Segmentation')\n",
    "\n",
    "# Classification Plots\n",
    "cp = pd.read_parquet('../../results/fabwave_classification.parquet')\n",
    "cp = cp[(cp.label != 14) & (cp.label != 22)]\n",
    "\n",
    "plot_classification_accuracies(cp, 'FabWave', title='FabWave Classification')\n",
    "fabwave_accuracy_plot = plot_classification_accuracies(cp, 'FabWave', title='FabWave Classification')\n",
    "\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "altair_saver.save(f360_accuracy_plot, os.path.join(outdir, 'f360-accuracy-plot.pdf'))\n",
    "altair_saver.save(mfcad_accuracy_plot, os.path.join(outdir, 'mfcad-accuracy-plot.pdf'))\n",
    "altair_saver.save(fabwave_accuracy_plot, os.path.join(outdir, 'fabwave-accuracy-plot.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from rendering import find_best_angle_from_part, render_part, render_mesh\n",
    "from tqdm import tqdm\n",
    "from render_shape import preds_to_mesh\n",
    "from zipfile import ZipFile\n",
    "import json\n",
    "from automate import Part\n",
    "from train_latent_space import BRepFaceAutoencoder\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "datasets = os.path.join(repbrep, 'datasets')\n",
    "model_dir = os.path.join(repbrep, 'models', 'BRepFaceAutoencoder_64_1024_4')\n",
    "results_dir = os.path.join(repbrep, 'results')\n",
    "\n",
    "f360seg_index_path = os.path.join(datasets, 'fusion360seg.json')\n",
    "f360seg_zip_path = os.path.join(datasets, 'fusion360seg.zip')\n",
    "model_checkpoint_path = os.path.join(model_dir, 'BRepFaceAutoencoder_64_1024_4.ckpt')\n",
    "computed_f360seg_codes_path = os.path.join(model_dir, 'fusion360seg_coded.pt')\n",
    "render_losses_path = os.path.join(results_dir, 'f360_render_test_losses.pt')\n",
    "figure_out_path = os.path.join(outdir, 'datasetgallery.png')\n",
    "\n",
    "train_poses_path = os.path.join(datasets, 'f360seg_test_poses.npy')\n",
    "train_zooms_path = os.path.join(datasets, 'f360seg_test_zooms.npy')\n",
    "\n",
    "rows = 4\n",
    "cols = 6\n",
    "size = 5\n",
    "\n",
    "grid_density = 100\n",
    "seed = 42\n",
    "\n",
    "\n",
    "with open(f360seg_index_path,'r') as f:\n",
    "    index = json.load(f)\n",
    "data =  ZipFile(f360seg_zip_path,'r')\n",
    "parts_list = [index['template'].format(*x) for x in index['test']]\n",
    "\n",
    "np.random.seed(42)\n",
    "to_render_idx = np.random.choice(np.arange(len(parts_list)), rows*cols, replace=False)\n",
    "\n",
    "gts = []\n",
    "\n",
    "poses = np.load(train_poses_path)\n",
    "zooms = np.load(train_zooms_path)\n",
    "\n",
    "\n",
    "\n",
    "for k in tqdm(range(rows*cols)):\n",
    "    i = to_render_idx[k]\n",
    "    path = parts_list[k]\n",
    "    part = Part(data.open(path).read().decode('utf-8'))\n",
    "    pose = poses[i]\n",
    "    zoom = zooms[i]\n",
    "    ground_truth = render_part(part, pose, zoom)\n",
    "    gts.append(ground_truth)\n",
    "\n",
    "M = rows*cols\n",
    "s = size\n",
    "fig, axes = plt.subplots(int(M/cols), cols, figsize=(cols*s, s*int(M/cols)),gridspec_kw = {'wspace':0, 'hspace':0}, dpi=300)\n",
    "for i in range(M):\n",
    "    row = int(i / cols)\n",
    "    col = i % cols\n",
    "    axes[row,col].imshow(gts[i])\n",
    "    axes[row,col].axis('off')\n",
    "fig.savefig(figure_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Model Version\n",
    "import torch\n",
    "import numpy as np\n",
    "from rendering import find_best_angle_from_part, render_part, render_mesh\n",
    "from tqdm import tqdm\n",
    "from render_shape import preds_to_mesh\n",
    "from zipfile import ZipFile\n",
    "import json\n",
    "from automate import Part\n",
    "from train_latent_space import BRepFaceAutoencoder\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "datasets = os.path.join(repbrep, 'datasets')\n",
    "model_dir = os.path.join(repbrep, 'models', 'BRepFaceAutoencoder_64_1024_4')\n",
    "results_dir = os.path.join(repbrep, 'results')\n",
    "\n",
    "f360seg_index_path = os.path.join(datasets, 'fusion360seg.json')\n",
    "f360seg_zip_path = os.path.join(datasets, 'fusion360seg.zip')\n",
    "model_checkpoint_path = os.path.join(model_dir, 'BRepFaceAutoencoder_64_1024_4.ckpt')\n",
    "computed_f360seg_codes_path = os.path.join(model_dir, 'fusion360seg_coded.pt')\n",
    "render_losses_path = os.path.join(results_dir, 'f360_render_test_losses.pt')\n",
    "figure_out_path = os.path.join(outdir, 'datasetgallery.png')\n",
    "\n",
    "train_poses_path = os.path.join(datasets, 'f360seg_test_poses.npy')\n",
    "train_zooms_path = os.path.join(datasets, 'f360seg_test_zooms.npy')\n",
    "\n",
    "rows = 4\n",
    "cols = 6\n",
    "size = 5\n",
    "\n",
    "grid_density = 100\n",
    "seed = 42\n",
    "\n",
    "\n",
    "with open(f360seg_index_path,'r') as f:\n",
    "    index = json.load(f)\n",
    "data =  ZipFile(f360seg_zip_path,'r')\n",
    "parts_list = [index['template'].format(*x) for x in index['test']]\n",
    "\n",
    "np.random.seed(42)\n",
    "to_render_idx = np.random.choice(np.arange(len(parts_list)), rows*cols, replace=False)\n",
    "\n",
    "gts = []\n",
    "\n",
    "poses = np.load(train_poses_path)\n",
    "zooms = np.load(train_zooms_path)\n",
    "\n",
    "\n",
    "\n",
    "for k in tqdm(range(rows*cols)):\n",
    "    i = to_render_idx[k]\n",
    "    path = parts_list[k]\n",
    "    part = Part(data.open(path).read().decode('utf-8'))\n",
    "    pose = poses[i]\n",
    "    zoom = zooms[i]\n",
    "    ground_truth = render_part(part, pose, zoom)\n",
    "    gts.append(ground_truth)\n",
    "\n",
    "M = rows*cols\n",
    "s = size\n",
    "fig, axes = plt.subplots(int(M/cols), cols, figsize=(cols*s, s*int(M/cols)),gridspec_kw = {'wspace':0, 'hspace':0}, dpi=300)\n",
    "for i in range(M):\n",
    "    row = int(i / cols)\n",
    "    col = i % cols\n",
    "    axes[row,col].imshow(gts[i])\n",
    "    axes[row,col].axis('off')\n",
    "fig.savefig(figure_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(index['train'])+len(index['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction Gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from hybridbrep import get_camera_angle, get_norm_factors, render_segmented_mesh, RendererParams, grid_images\n",
    "#from rendering import find_best_angle_from_part, render_part, render_mesh\n",
    "from tqdm import tqdm\n",
    "from render_shape import preds_to_mesh\n",
    "from zipfile import ZipFile\n",
    "import json\n",
    "#from automate import Part\n",
    "from hybridbrep import GeneralConvEncDec, HPart, HybridPartDataset\n",
    "from automate import Part, PartOptions\n",
    "#from train_latent_space import BRepFaceAutoencoder\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "datasets = os.path.join(repbrep, 'datasets')\n",
    "model_dir = os.path.join(repbrep, 'models', 'BRepFaceAutoencoder_64_1024_4')\n",
    "results_dir = os.path.join(repbrep, 'results')\n",
    "\n",
    "figure_out_path = os.path.join(outdir, 'reconstructions.png')\n",
    "\n",
    "\n",
    "rows = 4\n",
    "cols = 6\n",
    "size = 5\n",
    "N = 100\n",
    "\n",
    "imsize=800\n",
    "\n",
    "grid_density = 100\n",
    "\n",
    "force_rendering = True\n",
    "\n",
    "print('Loading Model')\n",
    "ckpt_path = '/home/ben/Documents/research/repbrep/training_logs/reconstruction/new_with_edges/version_1/checkpoints/epoch=183-val_loss=0.002646.ckpt'\n",
    "ckpt = torch.load(ckpt_path)\n",
    "model = GeneralConvEncDec(64, 1024, 4)\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "ablations_path = '../../results/recon_ablations.parquet'\n",
    "ablations = pd.read_parquet(ablations_path)\n",
    "errors = ablations[ablations.model == 'New Network, New Data']\n",
    "\n",
    "print('Loading Index')\n",
    "index_path = '../../datasets/fusion360seg.json'\n",
    "data_path = '../../datasets/fusion360seg.zip'\n",
    "with open(index_path, 'r') as f:\n",
    "    index = json.load(f)\n",
    "\n",
    "print('Counting Faces')\n",
    "ds_test = HybridPartDataset('../../datasets/fusion360seg.json', '../../datasets/fusion360seg_hpart_fixed.zip', mode='test')\n",
    "num_faces = {i:ds_test[i].faces.shape[0] for i in range(len(ds_test))}\n",
    "\n",
    "print('Selecting Parts')\n",
    "errors['faces'] = np.vectorize(lambda x: num_faces[x])(errors.test_idx)\n",
    "errors['total_value'] = errors.value * errors.faces\n",
    "total_errors = errors.groupby('test_idx').agg({'value':sum, 'total_value':sum}).reset_index()\n",
    "total_errors['faces'] = np.vectorize(lambda x: num_faces[x])(total_errors.test_idx)\n",
    "# Selection Criteria: lowest overall amount of loss with at least 20 faces\n",
    "\n",
    "to_render_idx = total_errors[total_errors.faces > 20].sort_values('total_value',ascending=True).test_idx.values[:rows*cols]\n",
    "\n",
    "print('Rendering')\n",
    "gts = []\n",
    "renders = []\n",
    "with ZipFile(data_path, 'r') as zf:    \n",
    "    for idx in tqdm(to_render_idx):\n",
    "        key = index['template'].format(*index['test'][idx])\n",
    "        with zf.open(key, 'r') as f:\n",
    "            part_data = f.read().decode('utf-8')\n",
    "        data = ds_test[idx]\n",
    "        with torch.no_grad():\n",
    "            pred = model.grid_enc_dec(data, N)\n",
    "        V, F, C = preds_to_mesh(pred[0], N)\n",
    "        opts = PartOptions()\n",
    "        opts.set_quality = True\n",
    "        opts.quality = 0.001\n",
    "        part = Part(part_data, opts)\n",
    "\n",
    "        color_pallet = plt.get_cmap('tab20')(np.arange(num_faces[idx]))[:,:3]\n",
    "\n",
    "        camera_params = get_camera_angle(\n",
    "            part.mesh.V, part.mesh.F, part.mesh_topology.face_to_topology,\n",
    "            optimize='seg'\n",
    "        )\n",
    "\n",
    "        gt_im = render_segmented_mesh(\n",
    "            part.mesh.V, part.mesh.F, part.mesh_topology.face_to_topology, color_pallet,\n",
    "            camera_params=camera_params,\n",
    "            render_params=RendererParams(imsize,imsize)\n",
    "            )\n",
    "\n",
    "        norm_center, norm_scale = get_norm_factors(data.V.numpy())\n",
    "        \n",
    "        recon_im = render_segmented_mesh(\n",
    "            V, F, C, color_pallet, camera_params=camera_params,\n",
    "            norm_center=norm_center,\n",
    "            norm_scale=norm_scale,\n",
    "            render_params=RendererParams(imsize,imsize)\n",
    "        )\n",
    "\n",
    "        gts.append(gt_im)\n",
    "        renders.append(recon_im)\n",
    "\n",
    "image_rows = []\n",
    "for r in range(rows):\n",
    "    curr_row = []\n",
    "    for c in range(cols):\n",
    "        idx = r*cols+c\n",
    "        gt = gts[idx]\n",
    "        recon = renders[idx]\n",
    "        curr_row.append(gt)\n",
    "        curr_row.append(recon)\n",
    "    curr_row = np.stack(curr_row)\n",
    "    image_rows.append(curr_row)\n",
    "image_rows = np.stack(image_rows)\n",
    "\n",
    "image_grid = grid_images(image_rows)\n",
    "\n",
    "image = Image.fromarray(image_grid.astype(np.uint8))\n",
    "\n",
    "image.save(figure_out_path)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile('../../datasets/fusion360seg.zip','r') as zf:\n",
    "    with open('../../datasets/fusion360seg.json', 'r') as f:\n",
    "        f360_idx = json.load(f)\n",
    "    part_path = f360_idx['template'].format(*f360_idx['test'][to_render_idx[8]])\n",
    "    with zf.open(part_path, 'r') as f:\n",
    "        part_data = f.read().decode('utf-8')\n",
    "with open('test_part.stp','w') as f:\n",
    "    f.write(part_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greyscale Reconstruction for Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version Using New Rendering Code\n",
    "# Problems:\n",
    "#  - normalizing parts and reconstructions separately makes them different sizes\n",
    "#    - solution: have a non-normalized version of camera param finding\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from train_latent_space import BRepFaceAutoencoder\n",
    "from rendering import render_segmented_mesh, get_camera_angle, grid_images, RendererParams\n",
    "from render_shape import preds_to_mesh\n",
    "from zipfile import ZipFile\n",
    "import json\n",
    "from automate import Part, PartOptions\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "datasets = os.path.join(repbrep, 'datasets')\n",
    "model_dir = os.path.join(repbrep, 'models', 'BRepFaceAutoencoder_64_1024_4')\n",
    "\n",
    "f360seg_index_path = os.path.join(datasets, 'fusion360seg.json')\n",
    "f360seg_zip_path = os.path.join(datasets, 'fusion360seg.zip')\n",
    "model_checkpoint_path = os.path.join(model_dir, 'BRepFaceAutoencoder_64_1024_4.ckpt')\n",
    "computed_f360seg_codes_path = os.path.join(model_dir, 'fusion360seg_coded.pt')\n",
    "render_losses_path = os.path.join(results_dir, 'f360_render_test_losses.pt')\n",
    "figure_out_path = os.path.join(outdir,'greyscale_reconstructions.png')\n",
    "\n",
    "grid_density = 100 # Point Sampling Density Per Face\n",
    "\n",
    "rows = 4\n",
    "cols = 6\n",
    "imsize=800\n",
    "\n",
    "print('Loading and Filtering Losses')\n",
    "testing_losses = torch.load(render_losses_path)\n",
    "avg_losses = np.array([x[0].item() for x in testing_losses])\n",
    "part_sizes = np.array([x[1] for x in testing_losses])\n",
    "total_losses = avg_losses * part_sizes\n",
    "avg_sorted = sorted(enumerate(zip(avg_losses, part_sizes, total_losses)),key=lambda x: x[1][0])\n",
    "total_sorted = sorted(enumerate(zip(avg_losses, part_sizes, total_losses)),key=lambda x: x[1][2])\n",
    "total_filtered = [x for x in total_sorted if x[1][1] > 20]\n",
    "\n",
    "print('Loading Dataset')\n",
    "with open(f360seg_index_path,'r') as f:\n",
    "    index = json.load(f)\n",
    "data =  ZipFile(f360seg_zip_path,'r')\n",
    "parts_list = [index['template'].format(*x) for x in index['test']]\n",
    "\n",
    "print('Loading Model')\n",
    "model = BRepFaceAutoencoder(64,1024,4)\n",
    "ckpt = torch.load(model_checkpoint_path)\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "def predict(face_codes, model, N=grid_density):\n",
    "    n_faces = face_codes.shape[0]\n",
    "    line = torch.linspace(-0.1,1.1,N)\n",
    "    grid = torch.cartesian_prod(line, line)\n",
    "    grids = grid.repeat(n_faces,1)\n",
    "    indices = torch.arange(n_faces).repeat_interleave(N*N, dim=0)\n",
    "    with torch.no_grad():\n",
    "        indexed_codes = face_codes[indices]\n",
    "        uv_codes = torch.cat([grids, indexed_codes],dim=1)\n",
    "        preds = model.decoder(uv_codes)\n",
    "    return preds\n",
    "\n",
    "codes = torch.load(computed_f360seg_codes_path)\n",
    "\n",
    "gts = []\n",
    "renders = []\n",
    "\n",
    "renderer = None\n",
    "\n",
    "options = PartOptions()\n",
    "options.set_quality = True\n",
    "options.quality = 0.001\n",
    "\n",
    "for k in tqdm(range(rows*cols), 'Predicting and Rendering'):\n",
    "    i = total_filtered[k][0]\n",
    "    N = grid_density\n",
    "    preds = predict(codes[parts_list[i]]['x'], model, N)\n",
    "    V, F, C = preds_to_mesh(preds, N)\n",
    "    part = Part(data.open(parts_list[i]).read().decode('utf-8'), options)\n",
    "    \n",
    "    camera_params = get_camera_angle(\n",
    "        part.mesh.V, \n",
    "        part.mesh.F, \n",
    "        part.mesh_topology.face_to_topology,\n",
    "        optimize='seg'\n",
    "    )\n",
    "\n",
    "    n_faces = part.mesh_topology.face_to_topology.max() + 1\n",
    "\n",
    "    face_colors = np.ones((n_faces,3)).astype(int)*150\n",
    "\n",
    "    gt_im = render_segmented_mesh(\n",
    "        part.mesh.V, part.mesh.F, part.mesh_topology.face_to_topology, face_colors,\n",
    "        camera_params=camera_params,\n",
    "        render_params=RendererParams(imsize,imsize)\n",
    "        )\n",
    "    \n",
    "    recon_im = render_segmented_mesh(\n",
    "        V, F, C, face_colors, camera_params=camera_params,\n",
    "        render_params=RendererParams(imsize,imsize)\n",
    "    )\n",
    "\n",
    "    gts.append(gt_im)\n",
    "    renders.append(recon_im)\n",
    "\n",
    "image_rows = []\n",
    "for r in range(rows):\n",
    "    curr_row = []\n",
    "    for c in range(cols):\n",
    "        idx = r*cols+c\n",
    "        gt = gts[idx]\n",
    "        recon = renders[idx]\n",
    "        curr_row.append(gt)\n",
    "        curr_row.append(recon)\n",
    "    curr_row = np.stack(curr_row)\n",
    "    image_rows.append(curr_row)\n",
    "image_rows = np.stack(image_rows)\n",
    "\n",
    "image_grid = grid_images(image_rows)\n",
    "\n",
    "Image.fromarray(image_grid.astype(np.uint8)).save(figure_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIG GREYSCALE GROUND TRUTH\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from train_latent_space import BRepFaceAutoencoder\n",
    "from rendering import render_segmented_mesh, get_camera_angle, grid_images, RendererParams\n",
    "from render_shape import preds_to_mesh\n",
    "from zipfile import ZipFile\n",
    "import json\n",
    "from automate import Part, PartOptions\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "datasets = os.path.join(repbrep, 'datasets')\n",
    "\n",
    "f360seg_index_path = os.path.join(datasets, 'fusion360seg.json')\n",
    "f360seg_zip_path = os.path.join(datasets, 'fusion360seg.zip')\n",
    "figure_out_path = os.path.join(outdir,'greyscale_gallery.png')\n",
    "\n",
    "rows = 20\n",
    "cols = 20\n",
    "imsize=400\n",
    "\n",
    "\n",
    "print('Loading Dataset')\n",
    "with open(f360seg_index_path,'r') as f:\n",
    "    index = json.load(f)\n",
    "\n",
    "random_selection = np.random.choice(len(index['test']), 400, replace=False)\n",
    "\n",
    "gts = []\n",
    "\n",
    "renderer = None\n",
    "\n",
    "options = PartOptions()\n",
    "options.set_quality = True\n",
    "options.quality = 0.001\n",
    "\n",
    "with ZipFile(f360seg_zip_path,'r') as data:\n",
    "    for i in tqdm(random_selection, 'rendering images'):\n",
    "\n",
    "        part = Part(data.open(index['template'].format(*index['test'][i])).read().decode('utf-8'), options)\n",
    "        \n",
    "        camera_params = get_camera_angle(\n",
    "            part.mesh.V, \n",
    "            part.mesh.F, \n",
    "            part.mesh_topology.face_to_topology,\n",
    "            optimize='seg'\n",
    "        )\n",
    "        n_faces = part.mesh_topology.face_to_topology.max() + 1\n",
    "        face_colors = np.ones((n_faces,3)).astype(int)*150\n",
    "\n",
    "        gt_im = render_segmented_mesh(\n",
    "            part.mesh.V, part.mesh.F, part.mesh_topology.face_to_topology, face_colors,\n",
    "            camera_params=camera_params,\n",
    "            render_params=RendererParams(imsize,imsize)\n",
    "            )\n",
    "        \n",
    "        gts.append(gt_im)\n",
    "\n",
    "\n",
    "image_rows = []\n",
    "for r in range(20):\n",
    "    curr_row = []\n",
    "    for c in range(20):\n",
    "        gt = gts[20*r+c]\n",
    "        curr_row.append(gt)\n",
    "    curr_row = np.stack(curr_row)\n",
    "    image_rows.append(curr_row)\n",
    "image_rows = np.stack(image_rows)\n",
    "\n",
    "image_grid = grid_images(image_rows)\n",
    "\n",
    "Image.fromarray(image_grid.astype(np.uint8)).save(figure_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from hybridbrep import get_camera_angle, get_norm_factors, render_segmented_mesh, RendererParams, grid_images\n",
    "#from rendering import find_best_angle_from_part, render_part, render_mesh\n",
    "from tqdm import tqdm\n",
    "from render_shape import preds_to_mesh\n",
    "from zipfile import ZipFile\n",
    "import json\n",
    "#from automate import Part\n",
    "from hybridbrep import GeneralConvEncDec, HPart, HybridPartDataset\n",
    "from automate import Part, PartOptions\n",
    "#from train_latent_space import BRepFaceAutoencoder\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "datasets = os.path.join(repbrep, 'datasets')\n",
    "results_dir = os.path.join(repbrep, 'results')\n",
    "\n",
    "figure_out_path = os.path.join(outdir,'greyscale_reconstructions.png')\n",
    "\n",
    "\n",
    "rows = 4\n",
    "cols = 6\n",
    "size = 5\n",
    "N = 100\n",
    "\n",
    "imsize=800\n",
    "\n",
    "grid_density = 100\n",
    "\n",
    "force_rendering = True\n",
    "\n",
    "print('Loading Model')\n",
    "ckpt_path = '/home/ben/Documents/research/repbrep/training_logs/reconstruction/new_with_edges/version_1/checkpoints/epoch=183-val_loss=0.002646.ckpt'\n",
    "ckpt = torch.load(ckpt_path)\n",
    "model = GeneralConvEncDec(64, 1024, 4)\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "ablations_path = '../../results/recon_ablations.parquet'\n",
    "ablations = pd.read_parquet(ablations_path)\n",
    "errors = ablations[ablations.model == 'New Network, New Data']\n",
    "\n",
    "print('Loading Index')\n",
    "index_path = '../../datasets/fusion360seg.json'\n",
    "data_path = '../../datasets/fusion360seg.zip'\n",
    "with open(index_path, 'r') as f:\n",
    "    index = json.load(f)\n",
    "\n",
    "print('Counting Faces')\n",
    "ds_test = HybridPartDataset('../../datasets/fusion360seg.json', '../../datasets/fusion360seg_hpart_fixed.zip', mode='test')\n",
    "num_faces = {i:ds_test[i].faces.shape[0] for i in range(len(ds_test))}\n",
    "\n",
    "print('Selecting Parts')\n",
    "errors['faces'] = np.vectorize(lambda x: num_faces[x])(errors.test_idx)\n",
    "errors['total_value'] = errors.value * errors.faces\n",
    "total_errors = errors.groupby('test_idx').agg({'value':sum, 'total_value':sum}).reset_index()\n",
    "total_errors['faces'] = np.vectorize(lambda x: num_faces[x])(total_errors.test_idx)\n",
    "# Selection Criteria: lowest overall amount of loss with at least 20 faces\n",
    "\n",
    "to_render_idx = total_errors[total_errors.faces > 20].sort_values('total_value',ascending=True).test_idx.values[:rows*cols]\n",
    "\n",
    "print('Rendering')\n",
    "gts = []\n",
    "renders = []\n",
    "with ZipFile(data_path, 'r') as zf:    \n",
    "    for idx in tqdm(to_render_idx):\n",
    "        key = index['template'].format(*index['test'][idx])\n",
    "        with zf.open(key, 'r') as f:\n",
    "            part_data = f.read().decode('utf-8')\n",
    "        data = ds_test[idx]\n",
    "        with torch.no_grad():\n",
    "            pred = model.grid_enc_dec(data, N)\n",
    "        V, F, C = preds_to_mesh(pred[0], N)\n",
    "        opts = PartOptions()\n",
    "        opts.set_quality = True\n",
    "        opts.quality = 0.001\n",
    "        part = Part(part_data, opts)\n",
    "\n",
    "        n_faces = part.mesh_topology.face_to_topology.max() + 1\n",
    "\n",
    "        color_pallet = np.ones((n_faces,3)).astype(int)*150\n",
    "        \n",
    "        camera_params = get_camera_angle(\n",
    "            part.mesh.V, part.mesh.F, part.mesh_topology.face_to_topology,\n",
    "            optimize='seg'\n",
    "        )\n",
    "\n",
    "        gt_im = render_segmented_mesh(\n",
    "            part.mesh.V, part.mesh.F, part.mesh_topology.face_to_topology, color_pallet,\n",
    "            camera_params=camera_params,\n",
    "            render_params=RendererParams(imsize,imsize)\n",
    "            )\n",
    "\n",
    "        norm_center, norm_scale = get_norm_factors(data.V.numpy())\n",
    "        \n",
    "        recon_im = render_segmented_mesh(\n",
    "            V, F, C, color_pallet, camera_params=camera_params,\n",
    "            norm_center=norm_center,\n",
    "            norm_scale=norm_scale,\n",
    "            render_params=RendererParams(imsize,imsize)\n",
    "        )\n",
    "\n",
    "        gts.append(gt_im)\n",
    "        renders.append(recon_im)\n",
    "\n",
    "image_rows = []\n",
    "for r in range(rows):\n",
    "    curr_row = []\n",
    "    for c in range(cols):\n",
    "        idx = r*cols+c\n",
    "        gt = gts[idx]\n",
    "        recon = renders[idx]\n",
    "        curr_row.append(gt)\n",
    "        curr_row.append(recon)\n",
    "    curr_row = np.stack(curr_row)\n",
    "    image_rows.append(curr_row)\n",
    "image_rows = np.stack(image_rows)\n",
    "\n",
    "image_grid = grid_images(image_rows)\n",
    "\n",
    "image = Image.fromarray(image_grid.astype(np.uint8))\n",
    "\n",
    "image.save(figure_out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import altair_saver\n",
    "\n",
    "accs = pd.read_parquet(os.path.join(results_dir, 'fusion360seg_reconstruction_vs_accuracy.parquet'))\n",
    "accs['one'] = 1\n",
    "iscorr = np.vectorize(lambda x: 'correct' if x == 1 else 'incorrect')\n",
    "accs['correct'] = iscorr(accs.label)\n",
    "accs['logmse'] = np.log(accs.mse)\n",
    "\n",
    "\"\"\"\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "lim_plot = alt.Chart(accs[accs.mse < 1]).mark_bar().encode(\n",
    "    x=alt.X('logmse',bin=alt.Bin(maxbins=50)),\n",
    "    y=alt.Y('sum(one)', stack='normalize', axis=alt.Axis(format='%')),\n",
    "    color=alt.Color('label:N')\n",
    ").facet(row='train_size')\n",
    "\n",
    "altair_saver.save(lim_plot, os.path.join(outdir, 'limitations.pdf'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import altair_saver\n",
    "\n",
    "mse_losses = torch.load('../../results/f360_recon_mse_losses.pt')\n",
    "f360_segmentation = pd.read_parquet('../../results/f360_segmentation.parquet')\n",
    "loss_records = []\n",
    "for test_idx, losses in enumerate(mse_losses):\n",
    "    for face_idx, loss in enumerate(losses):\n",
    "        loss_records.append({\n",
    "            'test_idx':test_idx,\n",
    "            'face_idx':face_idx,\n",
    "            'mse_loss':loss.item()\n",
    "        })\n",
    "mse_losses = pd.DataFrame.from_records(loss_records)\n",
    "f360_segmentation = f360_segmentation.merge(mse_losses, on=['test_idx','face_idx'])\n",
    "f360_segmentation = f360_segmentation[f360_segmentation.model == 'Ours']\n",
    "f360_segmentation = f360_segmentation[f360_segmentation.mse_loss < 10]\n",
    "bins = pd.cut(np.log(f360_segmentation.mse_loss), 10)\n",
    "bin_left = np.vectorize(lambda x: x.left)(bins)\n",
    "bin_right = np.vectorize(lambda x: x.right)(bins)\n",
    "f360_segmentation['bin_left'] = bin_left\n",
    "f360_segmentation['bin_right'] = bin_right\n",
    "binned_acc = f360_segmentation.groupby(['train_size','bin_left', 'bin_right']).agg({'accuracy':'mean'}).reset_index()\n",
    "seg_v_recon = alt.Chart(binned_acc).mark_bar().encode(\n",
    "    x = alt.X('bin_left', title='Rasterization log MSE (binned)'),\n",
    "    x2 = alt.X2('bin_right', title=None),\n",
    "    y = alt.Y('accuracy', title='Segmentation Accuracy')\n",
    ").properties(title = 'Segmentation vs Rasterization Accuracy').configure(\n",
    "    bar=alt.BarConfig(stroke='lightgrey')\n",
    ")\n",
    "altair_saver.save(seg_v_recon, os.path.join(outdir, 'seg_v_recon.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import render_segmentation_comparisons_newplotting\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "f360_comparisons, f360_labels, f360_label_colors = render_segmentation_comparisons_newplotting(root=repbrep, seg_pred_path = '../../results/f360_segmentation.parquet')\n",
    "Image.fromarray(f360_comparisons.astype(np.uint8)).save(os.path.join(outdir, 'f360seg-comparisons.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(f360_comparisons.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rendering import make_legend\n",
    "from matplotlib import pyplot as plt\n",
    "fig, axes = plt.subplots(1,1, figsize=(20,2))\n",
    "make_legend((f360_label_colors * 255).astype(np.uint8), np.array([\n",
    "    \"ExtrudeSide\",\n",
    "    \"ExtrudeEnd\",\n",
    "    \"CutSide\",\n",
    "    \"CutEnd\",\n",
    "    \"Fillet\",\n",
    "    \"Chamfer\",\n",
    "    \"RevolveSide\",\n",
    "    \"RevolveEnd\"\n",
    "])[f360_labels], axes, ncol=4, fontsize=60)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import render_segmentation_comparisons_newplotting\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "mfcad_comparisons, mfcad_labels, mfcad_label_colors = render_segmentation_comparisons_newplotting(\n",
    "    root=repbrep,\n",
    "    dataset_name='mfcad',\n",
    "    camera_name='mfcad',\n",
    "    dataset='MFCAD',\n",
    "    max_labels=16,\n",
    "    seg_pred_path = '../../results/mfcad_segmentation.parquet'\n",
    ")\n",
    "Image.fromarray(mfcad_comparisons.astype(np.uint8)).save(os.path.join(outdir, 'mfcad-comparison.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(mfcad_comparisons.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rendering import make_legend\n",
    "from matplotlib import pyplot as plt\n",
    "fig, axes = plt.subplots(1,1, figsize=(20,4))\n",
    "\n",
    "\n",
    "\n",
    "old_list = ['rectangular_through_slot', 'triangular_through_slot',\n",
    "              'rectangular_passage', 'triangular_passage', '6sides_passage',\n",
    "              'rectangular_through_step', '2sides_through_step', 'slanted_through_step',\n",
    "              'rectangular_blind_step', 'triangular_blind_step',\n",
    "              'rectangular_blind_slot',\n",
    "              'rectangular_pocket', 'triangular_pocket', '6sides_pocket',\n",
    "              'chamfer', 'stock']\n",
    "\n",
    "new_list2 = [\n",
    "    \"chamfer\", \"through_hole\", \n",
    "    \"triangular_passage\", \n",
    "    \"rectangular_passage\",\n",
    "     \"6sides_passage\", \"triangular_through_slot\"\n",
    "     , \"rectangular_through_slot\"\n",
    "     , \"circular_through_slot\", \"rectangular_through_step\", \"2sides_through_step\", \n",
    "     \"slanted_through_step\", \"Oring\", \n",
    "     \"blind_hole\",\n",
    "      \"triangular_pocket\"\n",
    "      , \"rectangular_pocket\", \n",
    "      \"6sides_pocket\", \"circular_end_pocket\",\n",
    "       \"rectangular_blind_slot\", \n",
    "       \"v_circular_end_blind_slot\"\n",
    "       , \"h_circular_end_blind_slot\", \"triangular_blind_step\", \"circular_blind_step\",\n",
    "        \"rectangular_blind_step\", \"round\", \"stock\"]\n",
    "new_list = [x for x in new_list2 if x in old_list]\n",
    "\n",
    "make_legend((mfcad_label_colors * 255).astype(np.uint8), new_list, axes, ncol=4, fontsize=60)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../../datasets/mfcad.json', 'r') as f:\n",
    "    mfi = json.load(f)\n",
    "[mfi['test'][i] for i in [357, 395, 592, 42, 1430, 1098, 1505, 1529, 1389, 1276]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(mfi['test_labels'][i])for i in [357, 395, 592, 42, 1430, 1098, 1505, 1529, 1389, 1276]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfi['train_labels'][9632]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "ms = mfcad_segmentation\n",
    "test_labels_res = []\n",
    "test_labels_idx = []\n",
    "test_labels_seg = []\n",
    "with ZipFile('../../datasets/mfcad.zip','r') as zf:\n",
    "    for i in tqdm(range(len(mfi['test']))):\n",
    "        res_labels = ms[\n",
    "            (ms.test_idx == i) & (ms.model == 'Ours') & (ms.seed == 0) & (ms.train_size == 10)\n",
    "            ].sort_values('face_idx').label.values\n",
    "        with zf.open(mfi['label_template'].format(*mfi['test'][i])) as f:\n",
    "            seg_labels = np.array([int(x.strip()) for x in f.readlines()])\n",
    "        idx_labels = np.array(mfi['test_labels'][i])\n",
    "        test_labels_res.append(res_labels)\n",
    "        test_labels_idx.append(idx_labels)\n",
    "        test_labels_seg.append(seg_labels)\n",
    "res_match_idx = [all(lr == li) for lr,li in zip(test_labels_res, test_labels_idx)]\n",
    "res_match_seg = [all(lr == ls) for lr,ls in zip(test_labels_res, test_labels_seg)]\n",
    "idx_match_seg = [all(li == ls) for li,ls in zip(test_labels_idx, test_labels_seg)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with ZipFile('../../datasets/mfcad.zip','r') as zf:\n",
    "    with zf.open('MFCAD-master/dataset/step/2-8-8-11-11-23.face_truth','r') as f:\n",
    "        dat = pickle.load(f)\n",
    "    #for i in tqdm(range(len(mfi['train']))):\n",
    "    #    with zf.open(mfi['label_template'].format(*mfi['train'][i])) as f:\n",
    "    #        seg_labels = np.array([int(x.strip()) for x in f.readlines()])\n",
    "    #    idx_labels = np.array(mfi['train_labels'][i])\n",
    "    #    same = all(idx_labels == seg_labels)\n",
    "    #    if not same:\n",
    "    #        print(i)\n",
    "    #        print(idx_labels)\n",
    "    #        print(seg_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[['rectangular_through_slot', 'triangular_through_slot',\n",
    "              'rectangular_passage', 'triangular_passage', '6sides_passage',\n",
    "              'rectangular_through_step', '2sides_through_step', 'slanted_through_step',\n",
    "              'rectangular_blind_step', 'triangular_blind_step',\n",
    "              'rectangular_blind_slot',\n",
    "              'rectangular_pocket', 'triangular_pocket', '6sides_pocket',\n",
    "              'chamfer', 'stock'][i] for i in dat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(res_match_idx) / len(res_match_idx)\n",
    "sum(res_match_seg) / len(res_match_seg)\n",
    "sum(idx_match_seg) / len(idx_match_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_labels42 = mfi['test_labels'][42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_unique(x):\n",
    "    return len(np.unique(x))\n",
    "ms.groupby(['test_idx','face_idx']).agg({'label':n_unique}).reset_index().label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idx_labels42)\n",
    "print(res_labels42)\n",
    "print(seg_labels42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation Gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rendering import render_segmented_mesh, grid_images, RendererParams\n",
    "from util import ZippedDataset\n",
    "from automate import Part\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# Counted from image\n",
    "rowcol = np.array([[0,7],\n",
    "[1,11],\n",
    "[2,9],\n",
    "[4,3],\n",
    "[7,18],\n",
    "[10,2],\n",
    "[13,10],\n",
    "[15,14],\n",
    "[18,3],\n",
    "[18,15]])\n",
    "test_indices = rowcol.dot(np.array([[20],[1]])).flatten()\n",
    "\n",
    "ds = ZippedDataset(os.path.join(repbrep, 'datasets', 'fusion360seg'))\n",
    "\n",
    "paths = [ds.index['template'].format(*ds.index['test'][i]) for i in test_indices]\n",
    "parts = [Part(ds.zip.open(p,'r').read().decode('utf-8')) for p in paths]\n",
    "seg_preds = pd.read_parquet(os.path.join(results_dir, 'f360_segmentation.parquet'))\n",
    "\n",
    "seg_preds = seg_preds[\n",
    "    (seg_preds.dataset == 'Fusion360Seg') &\n",
    "    (seg_preds.model == 'Ours') &\n",
    "    (seg_preds.seed == 0) &\n",
    "    (seg_preds.train_size == 23266)\n",
    "]\n",
    "\n",
    "preds_and_labels = seg_preds.sort_values(['test_idx','face_idx']).groupby('test_idx').agg({'label':list,'prediction':list}).reset_index()\n",
    "\n",
    "preds = preds_and_labels.prediction.values[test_indices]\n",
    "labels = preds_and_labels.label.values[test_indices]\n",
    "\n",
    "color_pallet = plt.get_cmap('tab10')(np.arange(8))[:,:3]\n",
    "\n",
    "renders = []\n",
    "for part,label,pred in zip(parts,labels,preds):\n",
    "    V = part.mesh.V\n",
    "    F = part.mesh.F\n",
    "    F_id = part.mesh_topology.face_to_topology\n",
    "    l = np.array(label)\n",
    "    p = np.array(pred)\n",
    "    l = color_pallet[l]\n",
    "    p = color_pallet[p]\n",
    "    pred_im = render_segmented_mesh(V,F,F_id,p,render_params=RendererParams(800,800),camera_opt='seg')\n",
    "    label_im = render_segmented_mesh(V,F,F_id,l,render_params=RendererParams(800,800),camera_opt='seg')\n",
    "    renders.append(grid_images(np.stack([np.stack([label_im, pred_im])])))\n",
    "image_grid = grid_images(np.stack(renders).reshape((5,2,800,1600,4)))\n",
    "Image.fromarray(image_grid.astype(np.uint8)).save(os.path.join(outdir, 'segmentation_gallery.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset Gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRep Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automate import Part\n",
    "from rendering import render_segmented_mesh, RendererParams\n",
    "from util import arr2im\n",
    "\n",
    "part = Part(os.path.join(repbrep, 'datasets', 'figure_parts', 'philipsbolt.step'))\n",
    "V = part.mesh.V\n",
    "F = part.mesh.F\n",
    "F_id = part.mesh_topology.face_to_topology\n",
    "\n",
    "renders = []\n",
    "colors = np.stack([np.array([50,50,50,50])]*len(F_id))\n",
    "renders.append(np.stack([render_segmented_mesh(V,F,F_id,colors,transparent_bg=True)]))\n",
    "for i in tqdm(range(5)):\n",
    "    colors = np.stack([np.array([50,50,50,50])]*len(F_id))\n",
    "    colors[i] = [250,50,50,255]\n",
    "    renders.append(np.stack([render_segmented_mesh(V,F,F_id,colors,transparent_bg=True)]))\n",
    "arr2im(grid_images(np.stack(renders)))#.save(os.path.join(outdir, 'transparent_bolts.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(renders)):\n",
    "    arr2im(renders[i][0,:,:,:]).save(os.path.join(outdir, f'tb{i}.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import altair_saver\n",
    "\n",
    "edf = pd.read_parquet(os.path.join(results_dir, 'tf_events.parquet'))\n",
    "\n",
    "fv_edf = edf[\n",
    "    edf.metric.str.contains('val') & \n",
    "    edf.metric.str.contains('loss') & \n",
    "    (~edf.metric.str.contains('step')) &\n",
    "    (edf.dataset == 'f360seg')]\n",
    "\n",
    "def get_stop_times(x):\n",
    "    min_time = x.rel_time.values[x.value.values.argmin()]\n",
    "    max_time = x.rel_time.values[x.value.values.argmax()]\n",
    "    return pd.Series([min_time, max_time], index=['min_time','max_time'])\n",
    "stopping_times = fv_edf.groupby(\n",
    "    ['model','train_size','seed','metric']\n",
    "    ).apply(get_stop_times).reset_index()\n",
    "\n",
    "model_names = {\n",
    "    'ours':'Ours',\n",
    "    'uvnet':'UV-Net',\n",
    "    'brepnet':'BRepNet'\n",
    "}\n",
    "rename_models = np.vectorize(lambda x: model_names[x])\n",
    "stopping_times.model = rename_models(stopping_times.model)\n",
    "\n",
    "stopping_times.min_time = stopping_times.min_time / 60\n",
    "\n",
    "line = alt.Chart(stopping_times)\\\n",
    "    .mark_line()\\\n",
    "    .encode(\n",
    "        x=alt.X('train_size', scale=alt.Scale(type='linear'),axis=alt.Axis(title='Training Set Size')),\n",
    "        y=alt.Y('mean(min_time)', scale=alt.Scale(type='linear'),axis=alt.Axis(title='Training Time (minutes)')),\n",
    "        color=alt.Color('model',sort=['Ours','UV-Net','BRepNet'],legend=alt.Legend(title='Model'))\n",
    "    )\n",
    "band = alt.Chart(stopping_times).mark_errorband(extent='ci').encode(\n",
    "        x=alt.X('train_size', scale=alt.Scale(type='linear'),axis=alt.Axis(title='Training Set Size')),\n",
    "        y=alt.Y('min_time', scale=alt.Scale(type='linear'),axis=alt.Axis(title='Training Time (minutes)')),\n",
    "        color=alt.Color('model',sort=['Ours','UV-Net','BRepNet'],legend=alt.Legend(title='Model'))\n",
    "    )\n",
    "chart = (band + line).properties(title='Fusion 360 Segmentation Training Time vs Training Size')\n",
    "\n",
    "altair_saver.save(chart, os.path.join(outdir, 'f360seg-time.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import altair_saver\n",
    "\n",
    "edf = pd.read_parquet(os.path.join(results_dir, 'tf_events.parquet'))\n",
    "\n",
    "fv_edf = edf[\n",
    "    edf.metric.str.contains('val') & \n",
    "    edf.metric.str.contains('loss') & \n",
    "    (~edf.metric.str.contains('step')) &\n",
    "    (edf.dataset == 'fabwave')]\n",
    "\n",
    "def get_stop_times(x):\n",
    "    min_time = x.rel_time.values[x.value.values.argmin()]\n",
    "    max_time = x.rel_time.values[x.value.values.argmax()]\n",
    "    return pd.Series([min_time, max_time], index=['min_time','max_time'])\n",
    "stopping_times = fv_edf.groupby(\n",
    "    ['model','train_size','seed','metric']\n",
    "    ).apply(get_stop_times).reset_index()\n",
    "\n",
    "model_names = {\n",
    "    'ours':'Ours',\n",
    "    'uvnet':'UV-Net',\n",
    "    'brepnet':'BRepNet'\n",
    "}\n",
    "rename_models = np.vectorize(lambda x: model_names[x])\n",
    "stopping_times.model = rename_models(stopping_times.model)\n",
    "\n",
    "stopping_times.min_time = stopping_times.min_time / 60\n",
    "\n",
    "line = alt.Chart(stopping_times)\\\n",
    "    .mark_line()\\\n",
    "    .encode(\n",
    "        x=alt.X('train_size', scale=alt.Scale(type='linear'),axis=alt.Axis(title='Training Set Size')),\n",
    "        y=alt.Y('mean(min_time)', scale=alt.Scale(type='linear'),axis=alt.Axis(title='Training Time (minutes)')),\n",
    "        color=alt.Color('model',sort=['Ours','UV-Net','BRepNet'],legend=alt.Legend(title='Model'))\n",
    "    )\n",
    "band = alt.Chart(stopping_times).mark_errorband(extent='ci').encode(\n",
    "        x=alt.X('train_size', scale=alt.Scale(type='linear'),axis=alt.Axis(title='Training Set Size')),\n",
    "        y=alt.Y('min_time', scale=alt.Scale(type='linear'),axis=alt.Axis(title='Training Time (minutes)')),\n",
    "        color=alt.Color('model',sort=['Ours','UV-Net','BRepNet'],legend=alt.Legend(title='Model'))\n",
    "    )\n",
    "chart = (band + line).properties(title='Fusion 360 Segmentation Training Time vs Training Size')\n",
    "chart\n",
    "#altair_saver.save(chart, os.path.join(outdir, 'f360seg-time.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clipping Plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hybridbrep import HPart\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "N = 1000#00\n",
    "part_path = os.path.join(repbrep, 'datasets/figure_parts/philipsbolt.step')\n",
    "data = HPart(part_path, N, N, True).data\n",
    "u = data.surface_coords[0,:,0].numpy()\n",
    "v = data.surface_coords[0,:,1].numpy()\n",
    "#v = v.max() - v\n",
    "m = data.surface_samples[0,:,-1].T.numpy()\n",
    "\n",
    "((u_min, v_min), (u_max, v_max)) = data.surface_bounds[0].numpy()\n",
    "u_prime = ((u *(u_max - u_min) + u_min) - (2*np.pi))\n",
    "v_prime = (v * (v_max - v_min) + v_min)\n",
    "\n",
    "plt.scatter(u_prime,v_prime,c=(m <= 0))\n",
    "plt.colorbar()\n",
    "plt.gca().set_aspect('equal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(u_prime,v_prime,c=(m),s=.03,cmap='tab20')\n",
    "plt.colorbar()\n",
    "plt.xlim(0,2*np.pi)\n",
    "plt.ylim(-np.pi/2, np.pi/2)\n",
    "plt.gca().set_aspect('equal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(u,v,c=(m<=0),s=.2, cmap='tab20')\n",
    "plt.colorbar()\n",
    "#plt.xlim(0,2*np.pi)\n",
    "#plt.ylim(-np.pi/2, np.pi/2)\n",
    "plt.gca().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import meshplot as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz = data.surface_samples[0,:,:3].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.plot(xyz, c=m, shading={'point_size':.03})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_path_step = os.path.join(repbrep, 'datasets/frame_guide/fg1.step')\n",
    "part_path_x_t = os.path.join(repbrep, 'datasets/frame_guide/fg1.x_t')\n",
    "\n",
    "for frac in np.linspace(0,1.0,6):\n",
    "\n",
    "    data = HPart(part_path_x_t, 500, 5000, True, frac).data\n",
    "\n",
    "    u = data.surface_coords[11,:,0].numpy()\n",
    "    v = data.surface_coords[11,:,1].numpy()\n",
    "    #v = v.max() - v\n",
    "    m = data.surface_samples[11,:,-1].T.numpy()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(u,v,c=m)\n",
    "    plt.colorbar()\n",
    "    plt.title(f'Sampling with {frac} sorted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('hybridbrep')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6aa2ac8926c38c90b9dbe4656179f4fe9d7aa58f584c2a3569efb38019544a4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
